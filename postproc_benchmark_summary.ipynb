{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAxoAAABlCAYAAADK3JXbAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAEP0lEQVR4nO3csW6bVRzG4eMosUMSO2okFstemo1rYIOJmRHRiYGrCCv3gFhBYmJh5gK4hgyxZAmxxUlI0zYfQ1W2UH+f3uhw0udZ4+HVP46ln2xn1HVdVwAAAIJ2ag8AAACeHqEBAADECQ0AACBOaAAAAHFCAwAAiBMaAABAnNAAAADidrd50P39fVmv12U6nZbRaPTYmwAAgP+pruvKZrMp8/m87Ow8/L7FVqGxXq/LcrmMjQMAANq2Wq3KYrF48OdbhcZ0Oi2llPLZNz+U3fFBZtkH4PtnP9We0KTvnn9ce0Jzvv3lr9oTmvT7869qT2jOs/GvtSc06fOTr2tPaM5voz9qT2jSp5/8WXtCc35882XtCc25u7kuP7/44t9GeMhWofHu41K744OyNxEa25ru79We0KS9g3HtCc053PNcG2Lf61lvH439fQ5xtH9Ye0Jz9kf7tSc06ehwUntCc8avj2pPaNb7vlLhy+AAAECc0AAAAOKEBgAAECc0AACAOKEBAADECQ0AACBOaAAAAHFCAwAAiBMaAABAnNAAAADihAYAABAnNAAAgDihAQAAxAkNAAAgTmgAAABxQgMAAIgTGgAAQJzQAAAA4oQGAAAQJzQAAIA4oQEAAMQJDQAAIE5oAAAAcUIDAACIExoAAECc0AAAAOKEBgAAECc0AACAOKEBAADECQ0AACBOaAAAAHFCAwAAiBMaAABAnNAAAADihAYAABAnNAAAgDihAQAAxAkNAAAgTmgAAABxQgMAAIgTGgAAQJzQAAAA4oQGAAAQJzQAAIA4oQEAAMQJDQAAIE5oAAAAcUIDAACIExoAAECc0AAAAOKEBgAAECc0AACAOKEBAADECQ0AACBOaAAAAHFCAwAAiBMaAABAnNAAAADihAYAABAnNAAAgDihAQAAxAkNAAAgTmgAAABxQgMAAIgTGgAAQJzQAAAA4oQGAAAQJzQAAIA4oQEAAMQJDQAAIE5oAAAAcUIDAACIExoAAEDc7jYP6rqulFLK67ubRx3z1GxuX9We0KRXN3e1JzTn+pXn2hC3L72m9fV35+9ziKvb69oTmnM7uq09oUlX1y9rT2jO3Zur2hOac3fz9jXtXSM8ZNS97xGllPPz83J6eppZBgAANG+1WpXFYvHgz7d6R+Pk5KSUUsrFxUU5Pj7OLPsAXF5eluVyWVarVZnNZrXnNMHNhnG3/txsGHfrz82Gcbf+3GwYd+uv67qy2WzKfD7/z8dtFRo7O2+/ynF8fOwXMMBsNnO3ntxsGHfrz82Gcbf+3GwYd+vPzYZxt362efPBl8EBAIA4oQEAAMRtFRqTyaScnZ2VyWTy2HueFHfrz82Gcbf+3GwYd+vPzYZxt/7cbBh3ezxb/dcpAACAPnx0CgAAiBMaAABAnNAAAADihAYAABAnNAAAgDihAQAAxAkNAAAgTmgAAABx/wC1zLBKyFDw7QAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 1000x100 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch_scatter\n",
    "from tqdm import tqdm\n",
    "\n",
    "import optbayesexpt as obe\n",
    "import config_matplotlib\n",
    "import seaborn as sns\n",
    "from src.utils_general import prepare_sample\n",
    "\n",
    "cmap_global = sns.color_palette('deep')\n",
    "sns.palplot(cmap_global)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_pkl_data(fname):\n",
    "    with open(fname, 'rb') as f:\n",
    "        data_dict = pickle.load(f)\n",
    "    loss_abs = np.abs(data_dict['param_mean'] - data_dict['param_true'][:,None,:])\n",
    "    loss_rel = (np.abs(data_dict['param_mean'] - data_dict['param_true'][:,None,:])) / np.abs(data_dict['param_true'][:,None,:])\n",
    "    return loss_abs, loss_rel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_pkl_data_for_dict(fname):\n",
    "    with open(fname, 'rb') as f:\n",
    "        data_dict = pickle.load(f)\n",
    "    param_mean_raw = []\n",
    "    param_true = []\n",
    "    measured_settings = []\n",
    "    measured_values = []\n",
    "    for idx in data_dict.keys():\n",
    "        # param_mean_raw.append(data_dict[idx]['param_mean'][None])\n",
    "        param_mean_raw.append((data_dict[idx]['particles'] * data_dict[idx]['particle_weights'][:,None,:]).sum(axis=-1)[None])\n",
    "        param_true.append(data_dict[idx]['param_true'][None])\n",
    "        measured_settings.append(data_dict[idx]['measurement_settings'])\n",
    "        measured_values.append(data_dict[idx]['measurements'])\n",
    "    max_len = max([p.shape[1] for p in param_mean_raw])\n",
    "    param_mean = []\n",
    "    for p in param_mean_raw:\n",
    "        if p.shape[0] < max_len:\n",
    "            param_mean.append(np.concatenate([p, np.repeat(p[:,-1,None,:], max_len-p.shape[1], axis=1)], axis=1))\n",
    "        else:\n",
    "            param_mean.append(p)\n",
    "    param_mean = np.vstack(param_mean)\n",
    "    param_true = np.vstack(param_true)\n",
    "    loss_abs = np.abs(param_mean - param_true[:,None,:])\n",
    "    loss_rel = (np.abs(param_mean - param_true[:,None,:])) / np.abs(param_true[:,None,:])\n",
    "\n",
    "    # loss_abs = np.abs(data_dict['param_mean'] - data_dict['param_true'][:,None,:])\n",
    "    # loss_rel = (np.abs(data_dict['param_mean'] - data_dict['param_true'][:,None,:])) / np.abs(data_dict['param_true'][:,None,:])\n",
    "    return loss_abs, loss_rel, param_true, measured_settings, measured_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3])\n"
     ]
    }
   ],
   "source": [
    "Nb = 40\n",
    "noise_level_list = [0.5, 1.0, 2.0]\n",
    "pw_list = [0.1, 0.2]\n",
    "datadir = 'benchmarks_2023May01'\n",
    "task_labels = ['gd', 'baseline', 'sequential', 'random']\n",
    "run_labels = [f'RUN_{i+1}' for i in range(5)]\n",
    "scatter_mean_indices = torch.arange(len(task_labels)).repeat_interleave(len(run_labels))\n",
    "print(scatter_mean_indices)\n",
    "\n",
    "results = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "times = np.linspace(0, 3, 121)\n",
    "times_finer = np.linspace(0, 3, 601)\n",
    "data = torch.load(\"data/CrI3/20221110.pt\")\n",
    "X = data['param'][:,:2]\n",
    "Y = torch.cat((data['omega'], data['inten']), dim=1)\n",
    "\n",
    "indices_dict = torch.load(\"data_splitting/indices_42_800-100-100.pt\")\n",
    "test_indices = indices_dict['test']\n",
    "\n",
    "X_test = X[test_indices]\n",
    "Y_test = Y[test_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2 [00:00<?, ?it/s]/tmp/ipykernel_50897/3009307654.py:24: RuntimeWarning: divide by zero encountered in divide\n",
      "  loss_rel = (np.abs(param_mean - param_true[:,None,:])) / np.abs(param_true[:,None,:])\n",
      "/home/zhantao/Dropbox/SLAC/research/TopologicalSpinML/topo-spin-Sqt-ML-main/src/utils_convolution.py:19: NumbaExperimentalFeatureWarning: Use of isinstance() detected. This is an experimental feature.\n",
      "  if isinstance(t, (int, float)):\n",
      "OMP: Info #276: omp_set_nested routine deprecated, please use omp_set_max_active_levels instead.\n",
      "100%|██████████| 100/100 [01:19<00:00,  1.26it/s]\n",
      "/tmp/ipykernel_50897/3009307654.py:24: RuntimeWarning: divide by zero encountered in divide\n",
      "  loss_rel = (np.abs(param_mean - param_true[:,None,:])) / np.abs(param_true[:,None,:])\n",
      "100%|██████████| 100/100 [01:12<00:00,  1.38it/s]\n",
      "100%|██████████| 100/100 [01:03<00:00,  1.56it/s]\n",
      "100%|██████████| 100/100 [01:10<00:00,  1.42it/s]\n",
      "100%|██████████| 100/100 [01:18<00:00,  1.27it/s]\n",
      "100%|██████████| 100/100 [01:11<00:00,  1.40it/s]\n",
      "100%|██████████| 2/2 [07:23<00:00, 221.80s/it]\n"
     ]
    }
   ],
   "source": [
    "for pw in tqdm(pw_list):\n",
    "    for nl in noise_level_list:\n",
    "        fname_lst = [\n",
    "            os.path.join(datadir, f'{run}/bayesian_{task}_pw-{pw}_nl-{nl}_Nb-{Nb:d}.pkl') for task in task_labels for run in run_labels\n",
    "        ]\n",
    "        # print('\\n'.join(fname_lst))\n",
    "        # loss_abs = [read_pkl_data_for_dict(f)[0] for f in fname_lst]\n",
    "        loss_abs = []\n",
    "        param_true = []\n",
    "        measured_settings = []\n",
    "        measured_values = []\n",
    "        for f in fname_lst:\n",
    "            _loss_abs, _loss_rel, _param_true, _settings, _values = read_pkl_data_for_dict(f)\n",
    "            loss_abs.append(_loss_abs)\n",
    "            param_true.append(_param_true)\n",
    "            measured_settings.append(_settings)\n",
    "            measured_values.append(_values)\n",
    "        # loss_rel = [read_pkl_data_for_dict(f)[1] for f in fname_lst]\n",
    "        loss = torch.tensor(np.asarray(loss_abs))\n",
    "        param_true = torch.tensor(np.asarray(param_true))\n",
    "        loss_exp_mean = torch_scatter.scatter_mean(loss, scatter_mean_indices, dim=0)\n",
    "        loss_mean = loss_exp_mean.mean(dim=1)\n",
    "        loss_std = torch_scatter.scatter_std(loss, scatter_mean_indices, dim=0).mean(dim=1)\n",
    "\n",
    "        signals = np.zeros((20, 100, len(times)))\n",
    "        signals_finer = np.zeros((20, 100, len(times_finer)))\n",
    "        for i_sample in tqdm(range(len(X_test))):\n",
    "            x = X_test[i_sample]\n",
    "            y = Y_test[i_sample]\n",
    "            for i_r, result in enumerate(param_true[:,i_sample,:]):\n",
    "                _, _, gamma, amp, wid = result\n",
    "                amp_factor = (amp / y[2:].max()).item()\n",
    "                _, func_I_conv, func_I_noconv = prepare_sample(\n",
    "                    x, y, gamma, times_finer, pulse_width=pw, normalize_to_value=100, \n",
    "                    elas_amp_factor=amp_factor, elas_wid=wid, elas_amp_abs_max=10.)\n",
    "                signals[i_r, i_sample] = func_I_conv(times)\n",
    "                signals_finer[i_r, i_sample] = func_I_conv(times_finer)\n",
    "\n",
    "        results[(pw, nl)] = {\n",
    "            \"mean_loss_avg_over_runs_samples\": loss_mean,\n",
    "            \"std_loss_avg_over_runs_samples\": loss_std,\n",
    "            \"loss_avg_over_runs\": loss_exp_mean,\n",
    "            \"loss_full\": loss,\n",
    "            \"loss_indices\": scatter_mean_indices,\n",
    "            \"param_true\": param_true,\n",
    "            \"times\": torch.from_numpy(times),\n",
    "            \"signals\": torch.from_numpy(signals),\n",
    "            \"measured_settings\": measured_settings,\n",
    "            \"measured_values\": measured_values,\n",
    "            \"times_finer\": torch.from_numpy(times_finer),\n",
    "            \"signals_finer\": torch.from_numpy(signals_finer),\n",
    "        }\n",
    "torch.save(results, f\"{datadir}/summarized_results.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.12079443, 0.1671994, 0.16999525, 0.28774175]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[np.mean(results[(0.1,0.5)]['loss_avg_over_runs'][i_strat,:,-1,0].numpy()) for i_strat in range(4)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1208, 0.0682],\n",
       "        [0.1672, 0.1002],\n",
       "        [0.1700, 0.0954],\n",
       "        [0.2877, 0.1638]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[(0.1,0.5)]['loss_avg_over_runs'].mean(dim=1)[:,-1,:2]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('sqt')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "649c1f73af0dec2056c8393170d01f81c1a70dc64f17d916a7f458cc0a8c8d2c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
