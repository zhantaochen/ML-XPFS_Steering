{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\zhant\\.conda\\envs\\sqt\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAx8AAABlCAYAAAAs9V6fAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjYuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/P9b71AAAACXBIWXMAAA9hAAAPYQGoP6dpAAAEQUlEQVR4nO3csW4jVRiG4eMosUMS29pINJbdbDqugQ4qakoEFQVXEVruAdGCREVDzQVwDSliKRKii5OQze5mKFZLZ3bnKPsdTfI8rT3Wr9/2SK/G41HXdV0BAAD4wHZaDwAAADwN4gMAAIgQHwAAQIT4AAAAIsQHAAAQIT4AAIAI8QEAAETs1h54f39fLi4uynQ6LaPR6CFnAgAABqTrurLZbMpisSg7O9uvb1THx8XFRVmtVrWHAwAAj8x6vS7L5XLr49XxMZ1OSymlfPbtj2V3fFD7Mk/OD89+bj3CIH3//OPWIwzOd7/+3XqEQfrj+VetRxicZ+PfWo8wSJ8ff916hMH5ffRn6xEG6dNP/mo9wuD89PrL1iMMzt3Ndfnlmy/+a4RtquPj7U+tdscHZW8iPt7XdH+v9QiDtHcwbj3C4Bzu+azV2Hc+6+2jse9njaP9w9YjDM7+aL/1CIN0dDhpPcLgjF8dtR5hsN51O4YbzgEAgAjxAQAARIgPAAAgQnwAAAAR4gMAAIgQHwAAQIT4AAAAIsQHAAAQIT4AAIAI8QEAAESIDwAAIEJ8AAAAEeIDAACIEB8AAECE+AAAACLEBwAAECE+AACACPEBAABEiA8AACBCfAAAABHiAwAAiBAfAABAhPgAAAAixAcAABAhPgAAgAjxAQAARIgPAAAgQnwAAAAR4gMAAIgQHwAAQIT4AAAAIsQHAAAQIT4AAIAI8QEAAESIDwAAIEJ8AAAAEeIDAACIEB8AAECE+AAAACLEBwAAECE+AACACPEBAABEiA8AACBCfAAAABHiAwAAiBAfAABAhPgAAAAixAcAABAhPgAAgAjxAQAARIgPAAAgQnwAAAAR4gMAAIgQHwAAQIT4AAAAIsQHAAAQIT4AAIAI8QEAAESIDwAAIEJ8AAAAEeIDAACIEB8AAECE+AAAACLEBwAAECE+AACACPEBAABEiA8AACBCfAAAABHiAwAAiBAfAABAhPgAAAAixAcAABAhPgAAgIjd2gO7riullPLq7ubBhnkKNrcvW48wSC9v7lqPMDjXL33Waty+cE7r65/O97PG1e116xEG53Z023qEQbq6ftF6hMG5e33VeoTBubt5c0572wjbjLp3PWOLs7OzcnJyUnMoAADwCK3X67JcLrc+Xn3l4/j4uJRSyvn5eZnP57Uv86RcXl6W1WpV1ut1mc1mrccZDHvrz87q2Ft/dlbH3vqzszr21p+d1em6rmw2m7JYLP73edXxsbPz5naR+XzujelpNpvZWQV768/O6thbf3ZWx976s7M69tafnfX3Phck3HAOAABEiA8AACCiOj4mk0k5PT0tk8nkIed51Oysjr31Z2d17K0/O6tjb/3ZWR1768/OPqzqf7sCAADow8+uAACACPEBAABEiA8AACBCfAAAABHiAwAAiBAfAABAhPgAAAAixAcAABDxL0qLsErUlwVEAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 1000x100 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch_scatter\n",
    "from tqdm import tqdm\n",
    "\n",
    "import optbayesexpt as obe\n",
    "import config_matplotlib\n",
    "import seaborn as sns\n",
    "from src.utils_general import prepare_sample\n",
    "\n",
    "cmap_global = sns.color_palette('deep')\n",
    "sns.palplot(cmap_global)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_pkl_data(fname):\n",
    "    with open(fname, 'rb') as f:\n",
    "        data_dict = pickle.load(f)\n",
    "    loss_abs = np.abs(data_dict['param_mean'] - data_dict['param_true'][:,None,:])\n",
    "    loss_rel = (np.abs(data_dict['param_mean'] - data_dict['param_true'][:,None,:])) / np.abs(data_dict['param_true'][:,None,:])\n",
    "    return loss_abs, loss_rel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_pkl_data_for_dict(fname):\n",
    "    with open(fname, 'rb') as f:\n",
    "        data_dict = pickle.load(f)\n",
    "    param_mean_raw = []\n",
    "    param_true = []\n",
    "    measured_settings = []\n",
    "    measured_values = []\n",
    "    for idx in data_dict.keys():\n",
    "        # param_mean_raw.append(data_dict[idx]['param_mean'][None])\n",
    "        param_mean_raw.append((data_dict[idx]['particles'] * data_dict[idx]['particle_weights'][:,None,:]).sum(axis=-1)[None])\n",
    "        param_true.append(data_dict[idx]['param_true'][None])\n",
    "        measured_settings.append(data_dict[idx]['measurement_settings'])\n",
    "        measured_values.append(data_dict[idx]['measurements'])\n",
    "    max_len = max([p.shape[1] for p in param_mean_raw])\n",
    "    param_mean = []\n",
    "    for p in param_mean_raw:\n",
    "        if p.shape[0] < max_len:\n",
    "            param_mean.append(np.concatenate([p, np.repeat(p[:,-1,None,:], max_len-p.shape[1], axis=1)], axis=1))\n",
    "        else:\n",
    "            param_mean.append(p)\n",
    "    param_mean = np.vstack(param_mean)\n",
    "    param_true = np.vstack(param_true)\n",
    "    loss_abs = np.abs(param_mean - param_true[:,None,:])\n",
    "    loss_rel = (np.abs(param_mean - param_true[:,None,:])) / np.abs(param_true[:,None,:])\n",
    "\n",
    "    # loss_abs = np.abs(data_dict['param_mean'] - data_dict['param_true'][:,None,:])\n",
    "    # loss_rel = (np.abs(data_dict['param_mean'] - data_dict['param_true'][:,None,:])) / np.abs(data_dict['param_true'][:,None,:])\n",
    "    return loss_abs, loss_rel, param_true, measured_settings, measured_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3])\n"
     ]
    }
   ],
   "source": [
    "Nb = 40\n",
    "noise_level_list = [0.5, 1.0, 2.0]\n",
    "pw_list = [0.1, 0.2]\n",
    "datadir = 'benchmarks_2023Apr05'\n",
    "task_labels = ['gd', 'baseline', 'sequential', 'random']\n",
    "run_labels = [f'RUN_{i+1}' for i in range(5)]\n",
    "scatter_mean_indices = torch.arange(len(task_labels)).repeat_interleave(len(run_labels))\n",
    "print(scatter_mean_indices)\n",
    "\n",
    "results = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "times = np.linspace(0, 3, 121)\n",
    "times_finer = np.linspace(0, 3, 601)\n",
    "data = torch.load(\"data/CrI3/20221110.pt\")\n",
    "X = data['param'][:,:2]\n",
    "Y = torch.cat((data['omega'], data['inten']), dim=1)\n",
    "\n",
    "indices_dict = torch.load(\"data_splitting/indices_42_800-100-100.pt\")\n",
    "test_indices = indices_dict['test']\n",
    "\n",
    "X_test = X[test_indices]\n",
    "Y_test = Y[test_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2 [00:00<?, ?it/s]C:\\Users\\zhant\\AppData\\Local\\Temp\\ipykernel_13696\\3009307654.py:24: RuntimeWarning: divide by zero encountered in divide\n",
      "  loss_rel = (np.abs(param_mean - param_true[:,None,:])) / np.abs(param_true[:,None,:])\n",
      "c:\\Users\\zhant\\Dropbox\\SLAC\\research\\TopologicalSpinML\\topo-spin-Sqt-ML-main\\src\\utils_convolution.py:19: NumbaExperimentalFeatureWarning: \u001b[1m\u001b[1mUse of isinstance() detected. This is an experimental feature.\u001b[0m\u001b[0m\n",
      "  if isinstance(t, (int, float)):\n",
      "100%|██████████| 100/100 [00:57<00:00,  1.73it/s]\n",
      "C:\\Users\\zhant\\AppData\\Local\\Temp\\ipykernel_13696\\3009307654.py:24: RuntimeWarning: divide by zero encountered in divide\n",
      "  loss_rel = (np.abs(param_mean - param_true[:,None,:])) / np.abs(param_true[:,None,:])\n",
      "100%|██████████| 100/100 [00:46<00:00,  2.13it/s]\n",
      "100%|██████████| 100/100 [00:47<00:00,  2.12it/s]\n",
      "100%|██████████| 100/100 [00:49<00:00,  2.00it/s]\n",
      "100%|██████████| 100/100 [00:48<00:00,  2.04it/s]\n",
      "100%|██████████| 100/100 [00:49<00:00,  2.02it/s]\n",
      "100%|██████████| 2/2 [05:09<00:00, 154.55s/it]\n"
     ]
    }
   ],
   "source": [
    "for pw in tqdm(pw_list):\n",
    "    for nl in noise_level_list:\n",
    "        fname_lst = [\n",
    "            os.path.join(datadir, f'{run}/bayesian_{task}_pw-{pw}_nl-{nl}_Nb-{Nb:d}.pkl') for task in task_labels for run in run_labels\n",
    "        ]\n",
    "        # print('\\n'.join(fname_lst))\n",
    "        # loss_abs = [read_pkl_data_for_dict(f)[0] for f in fname_lst]\n",
    "        loss_abs = []\n",
    "        param_true = []\n",
    "        measured_settings = []\n",
    "        measured_values = []\n",
    "        for f in fname_lst:\n",
    "            _loss_abs, _loss_rel, _param_true, _settings, _values = read_pkl_data_for_dict(f)\n",
    "            loss_abs.append(_loss_abs)\n",
    "            param_true.append(_param_true)\n",
    "            measured_settings.append(_settings)\n",
    "            measured_values.append(_values)\n",
    "        # loss_rel = [read_pkl_data_for_dict(f)[1] for f in fname_lst]\n",
    "        loss = torch.tensor(np.asarray(loss_abs))\n",
    "        param_true = torch.tensor(np.asarray(param_true))\n",
    "        loss_exp_mean = torch_scatter.scatter_mean(loss, scatter_mean_indices, dim=0)\n",
    "        loss_mean = loss_exp_mean.mean(dim=1)\n",
    "        loss_std = torch_scatter.scatter_std(loss, scatter_mean_indices, dim=0).mean(dim=1)\n",
    "\n",
    "        signals = np.zeros((20, 100, len(times)))\n",
    "        signals_finer = np.zeros((20, 100, len(times_finer)))\n",
    "        for i_sample in tqdm(range(len(X_test))):\n",
    "            x = X_test[i_sample]\n",
    "            y = Y_test[i_sample]\n",
    "            for i_r, result in enumerate(param_true[:,i_sample,:]):\n",
    "                _, _, gamma, amp, wid = result\n",
    "                amp_factor = (amp / y[2:].max()).item()\n",
    "                _, func_I_conv, func_I_noconv = prepare_sample(\n",
    "                    x, y, gamma, times_finer, pulse_width=pw, normalize_to_value=100, \n",
    "                    elas_amp_factor=amp_factor, elas_wid=wid, elas_amp_abs_max=10.)\n",
    "                signals[i_r, i_sample] = func_I_conv(times)\n",
    "                signals_finer[i_r, i_sample] = func_I_conv(times_finer)\n",
    "\n",
    "        results[(pw, nl)] = {\n",
    "            \"mean_loss_avg_over_runs_samples\": loss_mean,\n",
    "            \"std_loss_avg_over_runs_samples\": loss_std,\n",
    "            \"loss_avg_over_runs\": loss_exp_mean,\n",
    "            \"loss_full\": loss,\n",
    "            \"loss_indices\": scatter_mean_indices,\n",
    "            \"param_true\": param_true,\n",
    "            \"times\": torch.from_numpy(times),\n",
    "            \"signals\": torch.from_numpy(signals),\n",
    "            \"measured_settings\": measured_settings,\n",
    "            \"measured_values\": measured_values,\n",
    "            \"times_finer\": torch.from_numpy(times_finer),\n",
    "            \"signals_finer\": torch.from_numpy(signals_finer),\n",
    "        }\n",
    "torch.save(results, f\"{datadir}/summarized_results.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.13800006, 0.15725496, 0.2086613, 0.27846032]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[np.mean(results[(0.1,0.5)]['loss_avg_over_runs'][i_strat,:,-1,0].numpy()) for i_strat in range(4)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1380, 0.0770],\n",
       "        [0.1573, 0.0943],\n",
       "        [0.2087, 0.1199],\n",
       "        [0.2785, 0.1569]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[(0.1,0.5)]['loss_avg_over_runs'].mean(dim=1)[:,-1,:2]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('sqt')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "649c1f73af0dec2056c8393170d01f81c1a70dc64f17d916a7f458cc0a8c8d2c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
